%BREADCRUMBS{ format="$name" separator=" Â» "}%
---+++!! Configuration Changes for S3/S3N
   1. Add the following to hadoop core-site.xml for S3 FileSystem
      <verbatim>
        <property>
          <name>fs.default.name</name>
          <value>s3://BUCKET</value>
        </property> 
        <property> 
          <name>fs.s3.awsAccessKeyId</name> 
          <value>ID</value> 
        </property> 
        <property> 
          <name>fs.s3.awsSecretAccessKey</name> 
          <value>SECRET</value> 
        </property
      </verbatim>
   1. Modify the file_path in scribe.conf to s3:// or s3n:// as applicable
   1. Also modify the hdfsurl in &lt;cluster&gt;&lt;/cluster&gt; of databus.xml to s3://&lt;BUCKET_NAME&gt; or s3n://&lt;BUCKET_NAME&gt;
Note:

   1. If you want to use *S3N FileSystem*, change the above to s3n i.e change to s3n://BUCKET , fs.s3n.awsAccessKeyId and fs.s3n.awsSecretAccessKey
   1. S3 filesystem requires you to dedicate a bucket for the filesystem - you should not use an existing bucket containing files, or write other files to the same bucket.

I had to pass the complete S3 url with Access and Secret Keys in hdfsurl definition of &lt;cluster&gt;

