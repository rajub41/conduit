 -----
 Dev setup installation
 -----
 -----
 -----

Dev setup installation



*Hadoop Setup


 Hadoop on cluster was setup using the instructions found in Cloudera {{{https://ccp.cloudera.com/display/CDHDOC/CDH3\+Deployment\+on\+a\+Cluster}Document}}.


 Both dev1 and dev2 machines have


 1. {{{./NameNode.html}NameNode}} Directory is found at /etc/hadoop/hdfs/namenode


 2. {{{./DataNode.html}DataNode}} Directory is found at /etc/hadoop/hdfs/datanode


 3. Configuration is found at /etc/hadoop/conf


 dev2 machines can run multiple datanodes


 1. {{{./DataNode.html}DataNode}}2 and Datanode3 Directories are at /etc/hadoop/hdfs/datanode2 and /etc/hadoop/hdfs/datanode3 respectively.


 2. Configurations are found at /etc/hadoop/conf2 and /etc/hadoop/conf3 respectively.


 <strong><em>sudo service hadoop-0.20-jobtracker start</em></strong>"


 <strong><em>sudo service hadoop-0.20-tasktracker start</em></strong>"


 <strong><em>sudo service hadoop-0.20-namenode start</em></strong>"


 <em><strong>sudo service hadoop-0.20-datanode start</strong></em>" and services for other 2 are hadoop\-0.20\-datanode2 and hadoop\-0.20\-datanode3



**Scribe setup


 Install scribe\-service\-hdfs\-orig package. This installs the scribe binary in location /usr/bin.


 1. We have a wrapper script which sets the hadoop library class path present in /usr/bin/scribe.sh which should be run in background


 2. Logs everything to /tmp/scribe.log.


 3. The configuration is present in /etc/scribe.conf.


 4. We also have scribe_ctrl.py script present in /usr/bin for stopping, status, reloading etc. for controlling scribe.


 NOTE : PLEASE START SCRIBE AS "conduit" USER



**Zookeeper setup


 Zookeeper is setup using the Cloudera {{{https://ccp.cloudera.com/display/CDHDOC/ZooKeeper\+Installation}Document}}. No changes are made from default configuration.


 Start "sudo service hadoop\-zookeeper\-server start"


 Stop "sudo service hadoop\-zookeeper\-server stop"



**Conduit setup


 Conduit is setup using the generated deb package using dpkg command. Use the default configuration files present in /etc/conduit/conf to /usr/local/conduit/conf. All the configurations are attached here. Edit for any changes. Remove Symlink /usr/local/conduit and symlink to new /usr/local/conduit\-newversion. Starting conduit using the following command


 cd /var/log/conduit


 . /usr/local/conduit/bin/conduit.sh start /usr/local/conduit/conf/conduit.cfg


 For Stopping


 /usr/local/conduit/bin/conduit.sh stop /usr/local/conduit/conf/conduit.cfg


 Logs are written to the cwd where conduit is started in the above case /var/log/conduit


 NOTE : PLEASE START CONDUIT AS "conduit" USER



**Exception Cron Job


 <a href="../sendmail.py"> here </a>


 1. greps for any exception in logs if found gets the stacktrace from next few statements, caches all the necessary lines


 2. checkpoints the file so that same exceptions are not sent again.


 3. constructs email using email library in python and adds the caches lines and attaches conduit.log to the email to user@



**Changing open file handle count


 To change the open file handle count to 10240, do the following


 

 [[I]] Add the following in /etc/security/limits.conf :

  [[a]] hdfs hard nofile 10240

  [[a]] hadoop hard nofile 10240

  [[a]] mapred hard nofile 10240

  [[a]] hadoop soft nofile 10240

  [[a]] hdfs soft nofile 10240

  [[a]] mapred soft nofile 10240



 [[I]] Add the following to /etc/pam.d/common\-session :

  [[a]] session required pam_limits.so



 [[I]] Logout and login back for the changes to get reflected.

 [[I]] Restart the daemons for the change to effect on them

 [[I]] Verify the limits by looking /proc/\<pid\>/limits

 []



